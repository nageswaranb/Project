# -*- coding: utf-8 -*-
"""SIMPLILEARN_HEALTHCARE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ASnQ0oPfPZI7oU1PdDCgl8rkvu2tXeHC

'''Problem Statement: 

ICMR wants to analyze different types of cancers, such as breast cancer, renal cancer, colon cancer, lung cancer,
 and prostate cancer becoming a cause of worry in recent years. They would like to identify the probable cause of
  these cancers in terms of genes responsible for each cancer type. This would lead us to early identification of
   each type of cancer reducing the fatality rate.'''

'''The input dataset contains 802 samples for the corresponding 802 people who have been detected with different
 types of cancer. Each sample contains expression values of more than 20K genes. Samples have one of the types
  of tumors: BRCA, KIRC, COAD, LUAD, and PRAD.'''

# BRCA cancer- breast cancer
# COAD cancer- common malignant tumor in the digestive tract  
# KIRC cancer- Kidney renal 
# LUAD cancer- Lung cancer 
# PRAD cancer- Parotid tumors are abnormal growths of cells (tumors) that form in  the parotid glands
"""

# download the dataset
!wget https://www.dropbox.com/sh/8q39v4rvo9hq7hy/AAAfAs9J12eevM_9_jPySJ1xa?dl=0

# unzip the file
!unzip -qq /content/AAAfAs9J12eevM_9_jPySJ1xa?dl=0

!pip install tensorflow==2.2.0

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
colors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']
vectorizer = np.vectorize(lambda x: colors[x % len(colors)])

import warnings
warnings.filterwarnings(action='ignore',category=DeprecationWarning)
warnings.filterwarnings(action='ignore',category=FutureWarning)

import sys
import csv

csv.field_size_limit(sys.maxsize)

label = pd.read_csv('/content/labels.csv', delimiter=',', engine='python')
data = pd.read_csv('/content/data.csv', delimiter=',', engine='python')
data.describe()

data.info()

"""Project Task: Week 1

Exploratory Data Analysis:
"""

# Merge both the datasets
master_data = pd.merge(data, label)
master_data.head()

master_data.info()

# check for null values
master_data.isnull().sum()

master_data.describe()

"""Plot the merged dataset as a hierarchically-clustered heatmap"""

heatmap_data = pd.pivot_table(master_data, index=['Class'])                       
heatmap_data.head()

sns.clustermap(heatmap_data)
plt.savefig('heatmap_with_Seaborn_clustermap_python.jpg',
            dpi=150, figsize=(8,12))

sns.clustermap(heatmap_data, figsize=(18,12))
plt.savefig('clustered_heatmap_with_dendrograms_Seaborn_clustermap_python.jpg',dpi=150)

"""FEATURE SELECTION

split the data into train and test to avoid data-leakage
"""

master_data.head(5)

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X = master_data.iloc[:,1:-1]  #independent columns
y = master_data.iloc[:,-1]    #target column

X.head(5)

y.head(5)

#apply SelectKBest class to extract top 10 best features
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns

featureScores

print(featureScores.nlargest(10000,'Score'))  #print 10 best features from 20531 features

"""Feature Importance"""

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(X,y)

print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers

#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh', figsize=(25,100))
plt.show()

"""Checking histogram to check if the data is normally distributed"""

plt.figure(figsize=(10,6))
plt.hist(master_data['Class'], color = "orange")
plt.show()

non_cat_data = master_data.drop(['Unnamed: 0'], axis=1)
non_cat_data

"""Checking for null values"""

nan_cols = [i for i in non_cat_data.columns if non_cat_data[i].isnull().any()]
nan_cols

# change the first column name
first_column = master_data.iloc[: , :1]
first_column.head(5)

"""Dimensionality Reduction:

Each sample has expression values for around 20K genes. However, it may not be necessary to include all 20K genes expression values to analyze each cancer type. Therefore, we will identify a smaller set of attributes which will then be used to fit multiclass classification models. So, the first task targets the dimensionality reduction using various techniques such as, PCA, LDA, and t-SNE.bold text

Project Task: Week 2

Clustering Genes and Samples:

Our next goal is to identify groups of genes that behave similarly across samples and identify the distribution of samples corresponding to each cancer type. Therefore, this task focuses on applying various clustering techniques, e.g., k-means, hierarchical and mean shift clustering, on genes and samples.

First, apply the given clustering technique on all genes to identify:

Genes whose expression values are similar across all samples

Genes whose expression values are similar across samples of each cancer type

Next, apply the given clustering technique on all samples to identify:

Samples of the same class (cancer type) which also correspond to the same cluster

Samples identified to be belonging to another cluster but also to the same class (cancer type)

PRINCIPAL COMPONENT ANALYSIS (pca) - https://www.youtube.com/watch?v=FgakZw6K1QQ

REFERENCE - 

https://www.youtube.com/watch?v=OFyyWcw2cyM

https://github.com/krishnaik06/Dimesnsionality-Reduction/blob/master/01-Principal%20Component%20Analysis.ipynb

**PCA Visualization**

As we've noticed before it is difficult to visualize high dimensional data, we can use PCA to find the first two principal components, and visualize the data in this new, two-dimensional space, with a single scatter-plot. Before we do this though, we'll need to scale our data so that each feature has a single unit variance.
"""

from sklearn.preprocessing import StandardScaler

data_w_o_y = master_data.drop(['Unnamed: 0', 'Class'], axis=1)
data_w_o_y.head(5)

data_w_o_y.values.shape

scaler = StandardScaler()
X_Scaled = scaler.fit_transform(data_w_o_y)
X_Scaled

master_data.head(5)

master_data

no_target_master_data = master_data.drop(['Class'], axis=1)
no_target_master_data.head(5)

no_target_master_data.set_index('Unnamed: 0', inplace=True)
no_target_master_data

no_target_master_data.T

from sklearn import preprocessing

# First center and scale the data
scaled_data = preprocessing.scale(no_target_master_data)

scaled_data

pca = PCA() # create a PCA object
pca.fit(scaled_data) # do the math
pca_data = pca.transform(scaled_data) # get PCA coordinates for scaled_data

"""scree plot"""

#The following code constructs the Scree plot
per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)
labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]

plt.figure(figsize=(80,48)) 
plt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)
plt.ylabel('Percentage of Explained Variance')
plt.xlabel('Principal Component')
plt.title('Scree Plot')
plt.show()

#the following code makes a fancy looking plot using PC1 and PC2
pca_df = pd.DataFrame(pca_data, columns=labels)
 
plt.scatter(pca_df.PC1, pca_df.PC2)
plt.title('My PCA Graph')
plt.xlabel('PC1 - {0}%'.format(per_var[0]))
plt.ylabel('PC2 - {0}%'.format(per_var[1]))
 
for sample in pca_df.index:
    plt.annotate(sample, (pca_df.PC1.loc[sample], pca_df.PC2.loc[sample]))
 
plt.show()

genes = no_target_master_data.columns.values.tolist()
genes

#########################
#
# Determine which genes had the biggest influence on PC1
#
#########################
 
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
## first, get the loading scores
loading_scores = pd.Series(pca.components_[0], index=genes)
## now sort the loading scores based on their magnitude
sorted_loading_scores = loading_scores.abs().sort_values(ascending=False)
 
# get the names of the top 10 genes
top_10_genes = sorted_loading_scores[0:10].index.values
 
## print the gene names and their scores (and +/- sign)
print(loading_scores[top_10_genes])

"""We can't choose top 10."""

# Import PCA from sklearn and define the n_components as 2 
from sklearn.decomposition import PCA
pca_with_2=PCA(n_components=2)

"""PCA with Scikit Learn uses a very similar process to other preprocessing functions that come with SciKit Learn. We instantiate a PCA object, find the principal components using the fit method, then apply the rotation and dimensionality reduction by calling transform().

We can also specify how many components we want to keep when creating the PCA object.
"""

two_master_data = master_data

three_master_data = master_data

# Define data 
df_pca = two_master_data.drop(['Unnamed: 0'], axis=1)
df_pca = df_pca.drop(['Class'], axis=1)
df_pca.head()

df_pca.values.shape

x_pca = df_pca.values

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_Scaled = scaler.fit_transform(x_pca)
X_Scaled

# Import PCA from sklearn and define the n_components as 2 
from sklearn.decomposition import PCA
pca_with_2=PCA(n_components=2)

#Perform fit transform on the scaled data
X_pca_with_2 = pca_with_2.fit_transform(X_Scaled)
X_pca_with_2.shape

X_pca_with_2

df_cat_data = three_master_data

df_cat_data.head(5)

df_cat_data['Class'] = df_cat_data['Class'].map({'PRAD': 1, 'LUAD': 2, 'BRCA': 3, 'KIRC': 4, 'COAD': 5}) 
df_cat_data = df_cat_data.drop(['Unnamed: 0'],axis=1)

# Put the data back on the 2 columns defined 
df_pca = pd.DataFrame(X_pca_with_2)
df_pca.columns = ['pca1','pca2']

# Add the convereted categorical data for 
df_pca['cancer_type']=df_cat_data['Class']
df_pca

# Present the data on the 5 clusters using seaborn maps 
sns.scatterplot(x='pca1',y='pca2', hue = 'cancer_type',data=df_pca)

"""# ***PCA with n_components=.995***"""

pca_with_995=PCA(.995)
X_pca_with_995 = pca_with_995.fit_transform(x_pca)

X_pca_with_995.shape

X_pca_with_995

df_pca_995 = pd.DataFrame(X_pca_with_995)
df_pca_995['cancer_type']=df_cat_data['Class']
df_pca_995

sns.scatterplot(x=0,y=1,hue = 'cancer_type', data=df_pca_995)

"""How to select the number of components"""

four_master_data = master_data

five_master_data = master_data

# Define data 
df4_pca = four_master_data.drop(['Unnamed: 0'], axis=1)
df4_pca = df4_pca.drop(['Class'], axis=1)
df4_pca.head()

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data_rescaled = scaler.fit_transform(df4_pca)

# 100% of variance
from sklearn.decomposition import PCA
pca_801 = PCA(n_components = 801)
pca_801.fit(data_rescaled)
X_pca_801 = pca_801.transform(data_rescaled)

print("Variance explained by 95 components : ", sum(pca_801.explained_variance_ratio_ * 100))

pca_801.explained_variance_ratio_ * 100

np.cumsum(pca_801.explained_variance_ratio_ * 100)

plt.plot(np.cumsum(pca_801.explained_variance_ratio_))
plt.xlabel("Number of components")
plt.ylabel("Explained Variance")
plt.savefig("elbow_plot.png", dpi=100)

pca_801_fit = PCA().fit(data_rescaled)

# % matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (12,6)

fig, ax = plt.subplots()
xi = np.arange(1, 802, step=1)
y = np.cumsum(pca_801_fit.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components')
plt.xticks(np.arange(0, 802, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative variance (%)')
plt.title('The number of components needed to explain variance')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)

ax.grid(axis='x')
plt.show()

# % matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (36,18)  # (56, 36)

# creating a dictionary
font = {'size': 30}
  
# using rc function
plt.rc('font', **font)

fig, ax = plt.subplots()
xi = np.arange(1, 802, step=1)
y = np.cumsum(pca_801_fit.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components', fontsize=30)

plt.xticks(np.arange(0, 802, step=25), rotation=90) #change from 0-based array index to 1-based human-readable label


plt.ylabel('Cumulative variance (%)', fontsize=30)
plt.title('The number of components needed to explain variance')

# plt.setp(xi.get_xticklabels(), rotation=30, horizontalalignment='right')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=24)

ax.grid(axis='x')
plt.show()
plt.savefig("clear_elbow_plot.png")

# % matplotlib inline
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (36,18)  # (56, 36)

# creating a dictionary
font = {'size': 30}
  
# using rc function
plt.rc('font', **font)

fig, ax = plt.subplots()
xi = np.arange(1, 802, step=1)
y = np.cumsum(pca_801_fit.explained_variance_ratio_)

plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')

plt.xlabel('Number of Components', fontsize=30)

plt.xticks(np.arange(0, 802, step=25), rotation=90) #change from 0-based array index to 1-based human-readable label


plt.ylabel('Cumulative variance (%)', fontsize=30)
plt.title('The number of components needed to explain variance')

# plt.setp(xi.get_xticklabels(), rotation=30, horizontalalignment='right')

plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=24)

ax.grid(axis='x')
plt.show()
plt.savefig("clear_elbow_plot.png", dpi=100)

"""Dimensionality reduction using TSNE

Reference - https://www.youtube.com/watch?v=NEaUSP4YerM&t=625s
"""

df_tsne_data = master_data
non_numeric = ['Unnamed: 0','Class']
df_tsne_data = df_tsne_data.drop(non_numeric, axis=1)
df_tsne_data

#import T-SNE from sklearn
from sklearn.manifold import TSNE
m = TSNE(learning_rate=50)

tnse_features = m.fit_transform(df_tsne_data)
tnse_features[1:4,:]

df_tsne_data['x'] = tnse_features[:,0]
df_tsne_data['y'] = tnse_features[:,1]

import seaborn as sns
sns.scatterplot(x='x',y='y',data=df_tsne_data)
plt.show()

df_tsne_data['cancer_type']=df_cat_data['Class']
sns.scatterplot(x='x',y='y',hue = 'cancer_type', data=df_tsne_data)
plt.show()

"""Dimensionality reduction using LDA

Reference - https://www.youtube.com/watch?v=azXCzI57Yfc

so doing LDA, because, there is a cluster which is distance and spread inside cluster. So LDA will explain both.
"""

df_lda = master_data.drop(['Unnamed: 0'], axis=1)
df_lda = df_lda.drop(['Class'], axis=1)
x_lda = df_lda
x_lda

x_lda.shape

y_lda = master_data['Class']
y_lda.values

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
x_r2 = lda.fit(x_lda,y_lda).transform(x_lda)

lda.explained_variance_ratio_

x_r3 = pd.DataFrame(data=x_r2)
x_r3['y']=y_lda
x_r3

sns.scatterplot(x=0,y=1,hue = 'y', data=x_r3)

"""KMEANS Clustering with PCA = 2"""

from sklearn.cluster import KMeans
clusters = KMeans(5, n_init = 5)
clusters.fit(X_pca_with_2)

clusters.labels_

pca_with_2_data_frame = pd.DataFrame(data=X_pca_with_2,columns=['pca1','pca2'])
pca_with_2_data_frame.head()

pca_with_2_data_frame['Cls_label'] = clusters.labels_
pca_with_2_data_frame['given_cancer_type'] = label.Class.values
pca_with_2_data_frame

clusters.cluster_centers_

kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)
pred_y = kmeans.fit_predict(X_pca_with_2)
plt.scatter(X_pca_with_2[:,0], X_pca_with_2[:,1])
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

"""# ***KMEANS Clustering with PCA = .995***"""

from sklearn.cluster import KMeans
clusters_995 = KMeans(5, n_init = 5)
clusters_995.fit(X_pca_with_995)
clusters_995.labels_

pca_with_995_data_frame = pd.DataFrame(data=X_pca_with_995)
pca_with_995_data_frame.head()
pca_with_995_data_frame['Cls_label'] = clusters.labels_
pca_with_995_data_frame['given_cancer_type'] = label.Class.values

pca_with_995_data_frame.head(5)

pca_with_995_data_frame.shape

kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)
pred_y = kmeans.fit_predict(X_pca_with_995)
plt.scatter(X_pca_with_995[:,0], X_pca_with_995[:,1])
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

"""# ***Elbow method***"""

ks = range(1, 10)
inertias = []
for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(df_pca_995)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
plt.plot(ks, inertias, '-o', color='black')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

"""Build Classification Models

BUILDING AND RUNNING MANY ALGORITHMS AT ONCE

Reference - https://www.youtube.com/watch?v=7uLzGRlXXDw&list=PL9z-nia3KLoTBbK5ATeZnWMejwnIlVEPa&index=11
"""

ml_x = x_lda
ml_y = y_lda
ml_x.shape,ml_y.shape
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(ml_x,ml_y,test_size=0.30,random_state=30)

ml_x.head(5)

ml_y.head(5)

from sklearn.metrics import accuracy_score
from sklearn.metrics import matthews_corrcoef
from sklearn.metrics import f1_score

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import make_scorer
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, classification_report, \
            precision_score, recall_score, f1_score, roc_auc_score, roc_curve # ConfusionMatrixDisplay

models = {
    "Decision Tree": DecisionTreeClassifier(max_depth=5), 
    "Support Vector Classifier":SVC(probability=True, kernel='rbf'),
    "Random Forest Classifier": RandomForestClassifier(n_estimators=100), 
    "Gaussian Naive Bayes": GaussianNB(),
    "K-Nearest Neighbour": KNeighborsClassifier(n_neighbors=5)
}


for i in range(len(list(models))):
  print(i)
  model = list(models.values())[i]
  model.fit(ml_x, ml_y) # train on LDA dataframe

  # Make predictions
  y_train_pred = model.predict(x_train)
  y_test_pred = model.predict(x_test)

  # Traning set performance
  model_train_accuracy = accuracy_score(y_train, y_train_pred)
  model_train_f1 = f1_score(y_train, y_train_pred, average='weighted')
  model_train_precision = precision_score(y_train, y_train_pred, average='micro')
  model_train_recall = recall_score(y_train, y_train_pred, average='micro')
  

 # Testing set performance
  model_test_accuracy = accuracy_score(y_test, y_test_pred)
  model_test_f1 = f1_score(y_test, y_test_pred, average='weighted')
  model_test_precision = precision_score(y_test, y_test_pred, average='micro')
  model_test_recall = recall_score(y_test, y_test_pred, average='micro')
  
  print(list(models.keys())[i])

  # Model Train
  print('Model performance for Traning set')
  print("- Accuracy: {:.4f}".format(model_train_accuracy))
  print("- F1 Score: {:.4f}".format(model_train_f1))
  print("- Precision: {:.4f}".format(model_train_precision))
  print("- Recall: {:.4f}".format(model_train_recall))

  print("---------------------------------------------------------------")

  # Model Test
  print('Model performance for Testing set')
  print("- Accuracy: {:.4f}".format(model_test_accuracy))
  print("- F1 Score: {:.4f}".format(model_test_f1))
  print("- Precision: {:.4f}".format(model_test_precision))
  print("- Recall: {:.4f}".format(model_test_recall))


  print("="*35)
  print("\n")

rf_params = {
             "max_depth": [5, 8, 15, None, 10], 
             "max_features": [5, 7, "auto", 8], 
             "min_samples_split": [2, 8, 15, 20], 
             "n_estimators": [100, 200, 500, 1000], 
             }

rf_params

randomcv_models = [
                  ("RF", RandomForestClassifier(), rf_params)                   
                   ]

randomcv_models

from sklearn.model_selection import RandomizedSearchCV

model_param = {}
for name, model, params in randomcv_models:
  random = RandomizedSearchCV(estimator=model,
                              param_distributions=params,
                              n_iter=100,
                              cv=3,
                              verbose=2,
                              n_jobs=-1)
  random.fit(x_train, y_train)
  model_param[name] = random.best_params_

for model_name in model_param:
  print(f"--------------- Best param for {model_name} ---------------")
  print(model_param[model_name])

models = {
    "Random Forest": RandomForestClassifier(n_estimators=1000, min_samples_split = 15 , 
                                           max_features = "auto", max_depth = None)
}

for i in range(len(list(models))):
  model = list(models.values())[i]
  model.fit(x_train, y_train) # Train Model

  # Make predictions
  y_train_pred = model.predict(x_train)
  y_test_pred = model.predict(x_test)


  # Training set performance
  model_train_accuracy = accuracy_score(y_train, y_train_pred)
  model_train_f1 = f1_score(y_train, y_train_pred, average='weighted')
  model_train_precision = precision_score(y_train, y_train_pred, average='micro')
  model_train_recall = recall_score(y_train, y_train_pred, average='micro')

  # Testing set performance
  model_test_accuracy = accuracy_score(y_test, y_test_pred)
  model_test_f1 = f1_score(y_test, y_test_pred, average='weighted')
  model_test_precision = precision_score(y_test, y_test_pred, average='micro')
  model_test_recall = recall_score(y_test, y_test_pred, average='micro')

  print(list(models.keys())[i])

  # Model Train
  print('Model performance for Traning set')
  print("- Accuracy: {:.4f}".format(model_train_accuracy))
  print("- F1 Score: {:.4f}".format(model_train_f1))
  print("- Precision: {:.4f}".format(model_train_precision))
  print("- Recall: {:.4f}".format(model_train_recall))

  print("---------------------------------------------------------------")

  # Model Test
  print('Model performance for Testing set')
  print("- Accuracy: {:.4f}".format(model_test_accuracy))
  print("- F1 Score: {:.4f}".format(model_test_f1))
  print("- Precision: {:.4f}".format(model_test_precision))
  print("- Recall: {:.4f}".format(model_test_recall))


  print("="*35)
  print("\n")

"""# ***DEEP NEURAL NETWORK***"""

!wget https://www.dropbox.com/sh/8q39v4rvo9hq7hy/AAAfAs9J12eevM_9_jPySJ1xa?dl=0

!unzip -qq /content/AAAfAs9J12eevM_9_jPySJ1xa?dl=0

!pip install scikit-learn==0.21.2

!pip install tensorflow==2.2.0

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
colors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']
vectorizer = np.vectorize(lambda x: colors[x % len(colors)])

import warnings
warnings.filterwarnings(action='ignore',category=DeprecationWarning)
warnings.filterwarnings(action='ignore',category=FutureWarning)

import sys
import csv

csv.field_size_limit(sys.maxsize)

import tensorflow as tf
tf.test.gpu_device_name()

from tensorflow.python.client import device_lib
device_lib.list_local_devices()

!cat /proc/meminfo

label = pd.read_csv('/content/labels.csv', delimiter=',', engine='python')
data = pd.read_csv('/content/data.csv', delimiter=',', engine='python')
data.describe()

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dropout

# Merge both the datasets
master_data = pd.merge(data, label)
master_data.head()

deep_learning_data = master_data

features=deep_learning_data.drop(['Unnamed: 0'],axis=1)
features=features.drop(['Class'],axis=1)
target=deep_learning_data['Class']
features.head(5)

target.head(5)

x=features
x.head(5)

y = deep_learning_data['Class']

y.head(5)

(trainX, testX, trainY, testY) = train_test_split(x, y, test_size=0.20, random_state=42)

master_data.shape

deep_learning_data.shape

print(x.shape)
print(y.shape)

trainX.head(5)

trainY.head(5)

trainX.shape, testX.shape, trainY.shape, testY.shape

from sklearn.preprocessing import LabelBinarizer

lb = LabelBinarizer()

trainY = lb.fit_transform(trainY)
testY = lb.transform(testY)

trainY

"""# param_number = output_channel_number * (input_channel_number * kernel_height * kernel_width + 1)

# param_number = output_channel_number * (input_channel_number + 1) *italicized text*
"""

import tensorflow
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

def create_model(layers, activation):
    model = Sequential()
    for i, nodes in enumerate(layers):
        if i==0:
            model.add(Dense(nodes, input_dim=trainX.shape[1], kernel_initializer="uniform", kernel_regularizer=l2(0.0002)))
            model.add(Activation(activation))
            model.add(BatchNormalization())
            model.add(Dropout(0.25))
        else:
            model.add(Dense(nodes, kernel_initializer="uniform", kernel_regularizer=l2(0.0002)))
            model.add(Activation(activation))
            model.add(BatchNormalization())
            model.add(Dropout(0.25))   

    # model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid')) # Note: no activation beyond this point
    model.add(Dense(units=5, activation='softmax'))
    
    opt = Adam(lr=0.0001, beta_1=0.5, decay=0.0002 / 30)
    model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])
    return model

model = KerasClassifier(build_fn=create_model, verbose=0)

model

layers = [[20], [40, 20], [45, 30, 15], [50,35,20]]
activations = ['sigmoid', 'relu', 'elu']
param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])
grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=5)

grid_result = grid.fit(trainX, trainY, validation_data=(testX, testY)) # , batch_size=32, validation_data=(testX, testY), epochs=[30])

print([grid_result.best_score_,grid_result.best_params_])

tensorflow.keras.backend.clear_session()

# define the architecture of the network 
model = Sequential()
model.add(Dense(50, input_dim=trainX.shape[1], kernel_initializer="uniform", kernel_regularizer=l2(0.0002), name="LAYER____1"))
model.add(Activation("elu"))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(35, kernel_initializer="uniform", kernel_regularizer=l2(0.0002), name="LAYER____2"))
model.add(Activation("elu"))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(20, kernel_initializer="uniform", kernel_regularizer=l2(0.0002), name="LAYER____3"))
model.add(Activation("elu"))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(5, activation='softmax'))

model.summary()

# ADAM OPTIMIZER
print("[INFO] compiling model...")

NUM_EPOCHS=50

opt = Adam(lr=0.0001, beta_1=0.5, decay=0.0002 / NUM_EPOCHS)
model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])
H = model.fit(trainX, trainY, batch_size=128, 
                        validation_data=(testX, testY), epochs=NUM_EPOCHS, verbose=1)

# save the network to disk
print("[INFO] serializing network...")
model.save("/content/cluster_weights.hdf5")

from sklearn.metrics import classification_report

# evaluate the network
print("[INFO] evaluating network...")
predictions = model.predict(testX, batch_size=128)
print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1),
target_names=[str(x) for x in lb.classes_]))

xyz = model.predict(testX)
y_pr=[]
for k in xyz:
    y_pr.append(np.argmax(k))
    
y_val=[]
for k in testY:
    y_val.append(np.argmax(k))

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
confusion_matrix(y_val, y_pr)

# plot the training loss and accuracy
plt.style.use("ggplot")
plt.figure()

plt.plot(np.arange(0, 50), H.history["loss"], label="train_loss")
plt.plot(np.arange(0, 50), H.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, 50), H.history["accuracy"], label="train_acc")
plt.plot(np.arange(0, 50), H.history["val_accuracy"], label="val_acc")

plt.title("Training Loss and Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend()
plt.savefig("/content/train_val_graph.jpg")











