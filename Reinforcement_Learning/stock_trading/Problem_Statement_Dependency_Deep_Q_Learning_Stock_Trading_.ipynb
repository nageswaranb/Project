{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgy74k3DDaf_"
      },
      "source": [
        "#**Stock Trading Using Deep Q-Learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW3MPB3pDndR"
      },
      "source": [
        "## **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeOMTczqDqdZ"
      },
      "source": [
        "Prepare an agent by implementing Deep Q-Learning that can perform unsupervised trading in stock trade. The aim of this project is to train an agent that uses Q-learning and neural networks to predict the profit or loss by building a model and implementing it on a dataset that is available for evaluation.\n",
        "\n",
        "\n",
        "The stock trading index environment provides the agent with a set of actions:<br>\n",
        "* Buy<br>\n",
        "* Sell<br>\n",
        "* Sit\n",
        "\n",
        "This project has following sections:\n",
        "* Import libraries \n",
        "* Create a DQN agent\n",
        "* Preprocess the data\n",
        "* Train and build the model\n",
        "* Evaluate the model and agent\n",
        "<br><br>\n",
        "\n",
        "**Steps to perform**<br>\n",
        "\n",
        "In the section **create a DQN agent**, create a class called agent where:\n",
        "* Action size is defined as 3\n",
        "* Experience replay memory to deque is 1000\n",
        "* Empty list for stocks that has already been bought\n",
        "* The agent must possess the following hyperparameters:<br>\n",
        "  * gamma= 0.95<br>\n",
        "  * epsilon = 1.0<br>\n",
        "  * epsilon_final = 0.01<br>\n",
        "  * epsilon_decay = 0.995<br>\n",
        "\n",
        "\n",
        "    Note: It is advised to compare the results using different values in hyperparameters.\n",
        "\n",
        "* Neural network has 3 hidden layers\n",
        "* Action and experience replay are defined\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu4reAtsL5EZ"
      },
      "source": [
        "## **Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset **"
      ],
      "metadata": {
        "id": "kWW3OG1mV4dU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swIl-jUpWAA8",
        "outputId": "c10626db-4531-475c-d021-2fb21346a848"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq /content/drive/MyDrive/datasets/simplilearn_RL_stock_trading/dataset.zip"
      ],
      "metadata": {
        "id": "MU7LoSirWX3E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!scp /content/dataset/GSPC_Training_Dataset.csv /content/GSPC_Training_Dataset.csv\n",
        "!scp /content/dataset/GSPC_Evaluation_Dataset.csv /content/GSPC_Evaluation_Dataset.csv"
      ],
      "metadata": {
        "id": "nAJnJXqRMiW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REINFORCEMENT LIBRARIES**"
      ],
      "metadata": {
        "id": "3gDPqDQvlkDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-opengl xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!apt install xvfb -y\n",
        "!pip install piglet\n",
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]\n",
        "!pip install tensorflow==2.3.1 gym keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MLVUttvsln-d",
        "outputId": "46ebc32a-9cd5-4f44-9232-2e98f024cd55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python-opengl (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for python-opengl\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 785 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.11 [785 kB]\n",
            "Fetched 785 kB in 0s (2,637 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 159447 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.11_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.11) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting piglet\n",
            "  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n",
            "Collecting piglet-templates\n",
            "  Downloading piglet_templates-1.3.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (22.1.0)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.9)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.1)\n",
            "Installing collected packages: piglet-templates, piglet\n",
            "Successfully installed piglet-1.0.0 piglet-templates-1.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "\u001b[33mWARNING: gym 0.25.2 does not provide the extra 'box_2d'\u001b[0m\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[Box_2D]) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[Box_2D]) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.3.1\n",
            "  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4 MB 26 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting h5py<2.11.0,>=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 44.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.17.3)\n",
            "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 75.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (2.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.37.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.48.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (3.3.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.1) (1.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym) (0.0.8)\n",
            "Installing collected packages: numpy, tensorflow-estimator, h5py, gast, tensorflow, keras-rl2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.15+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.17 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "cmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.3.3 h5py-2.10.0 keras-rl2-1.0.5 numpy-1.18.5 tensorflow-2.3.1 tensorflow-estimator-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBgbvVTRDXpe"
      },
      "source": [
        "### **Import the libraries** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt5QkvOCri3W"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "behdrRbIDXpj"
      },
      "source": [
        "### **Create a DQN agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrJH6vRmNZQw"
      },
      "source": [
        "**Use the instruction below to prepare an agent**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7uHLPJWtNmm"
      },
      "source": [
        "# Action space include 3 actions: Buy, Sell, and Sit\n",
        "#Setting up the experience replay memory to deque with 1000 elements inside it\n",
        "#Empty list with inventory is created that contains the stocks that were already bought\n",
        "#Setting up gamma to 0.95, that helps to maximize the current reward over the long-term\n",
        "#Epsilon parameter determines whether to use a random action or to use the model for the action. \n",
        "#In the beginning random actions are encouraged, hence epsilon is set up to 1.0 when the model is not trained.\n",
        "#And over time the epsilon is reduced to 0.01 in order to decrease the random actions and use the trained model\n",
        "#We're then set the speed of decreasing epsililon in the epsilon_decay parameter\n",
        "\n",
        "#Defining our neural network:\n",
        "#Define the neural network function called _model and it just takes the keyword self\n",
        "#Define the model with Sequential()\n",
        "#Define states i.e. the previous n days and stock prices of the days\n",
        "#Defining 3 hidden layers in this network\n",
        "#Changing the activation function to relu because mean-squared error is used for the loss\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nRItH8mDXpm"
      },
      "source": [
        "### **Preprocess the stock market data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The environment is given**"
      ],
      "metadata": {
        "id": "5BT9b-_GnTfO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3NGhJubtfet"
      },
      "source": [
        "import math\n",
        "\n",
        "# prints formatted price\n",
        "def formatPrice(n):\n",
        "\treturn (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
        "\n",
        "# returns the vector containing stock data from a fixed file\n",
        "def getStockDataVec(key):\n",
        "\tvec = []\n",
        "\tlines = open(\"\" + key + \".csv\", \"r\").read().splitlines()\n",
        "\n",
        "\tfor line in lines[1:]:\n",
        "\t\tvec.append(float(line.split(\",\")[4]))\n",
        "\n",
        "\treturn vec\n",
        "\n",
        "# returns the sigmoid\n",
        "def sigmoid(x):\n",
        "\treturn 1 / (1 + math.exp(-x))\n",
        "\n",
        "# returns an an n-day state representation ending at time t\n",
        "def getState(data, t, n):\n",
        "\td = t - n + 1\n",
        "\tblock = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "\tres = []\n",
        "\tfor i in range(n - 1):\n",
        "\t\tres.append(sigmoid(block[i + 1] - block[i]))\n",
        "\n",
        "\treturn np.array([res])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "z9M1ejobQNP3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changes\n",
        "class Agent():\n",
        "    def __init__(self, window_size, is_eval=False, model_name=''):\n",
        "\n",
        "        self.nS = window_size\n",
        "        self.nA = 3\n",
        "        self.memory = deque([], maxlen=1000)\n",
        "        self.alpha = 0.001\n",
        "        self.window_size = window_size\n",
        "        self.gamma = 0.95\n",
        "        #Explore/Exploit\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        # self.model = self.build_model()\n",
        "        self.loss = []\n",
        "\n",
        "        self.is_eval = is_eval\n",
        "        self.model = load_model(model_name) if self.is_eval else self.build_model()\n",
        "        \n",
        "    def build_model(self):\n",
        "        # model = keras.Sequential() \n",
        "        # model.add(keras.layers.Dense(24, input_dim=self.window_size, activation='relu')) #[Input] -> Layer 1\n",
        "        # #   Dense: Densely connected layer https://keras.io/layers/core/\n",
        "        # #   24: Number of neurons\n",
        "        # #   input_dim: Number of input variables\n",
        "        # #   activation: Rectified Linear Unit (relu) ranges >= 0\n",
        "        # model.add(keras.layers.Dense(24, activation='relu')) #Layer 2 -> 3\n",
        "        # model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 3 -> 4\n",
        "        # # model.add(keras.layers.Dense(self.nA, activation='linear')) #Layer 4 -> [output]\n",
        "        # #   Size has to match the output (different actions)\n",
        "        # #   Linear activation on the last layer\n",
        "        # model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n",
        "        #               optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Dense(50, input_dim=self.window_size, kernel_initializer=\"uniform\", kernel_regularizer=l2(0.0002), name=\"LAYER____1\"))\n",
        "        model.add(Activation(\"elu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Dense(35, kernel_initializer=\"uniform\", kernel_regularizer=l2(0.0002), name=\"LAYER____2\"))\n",
        "        model.add(Activation(\"elu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Dense(20, kernel_initializer=\"uniform\", kernel_regularizer=l2(0.0002), name=\"LAYER____3\"))\n",
        "        model.add(Activation(\"elu\"))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Dense(self.nA, activation='linear'))\n",
        "\n",
        "        opt = Adam(lr=0.0001, beta_1=0.5, decay=0.0002)\n",
        "        model.compile(loss=\"mean_squared_error\", optimizer=opt)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def act(self, state):#act\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(3) #Explore\n",
        "        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n",
        "        return np.argmax(action_vals[0])      \n",
        "\n",
        "    def test_action(self, state): #Exploit\n",
        "        action_vals = self.model.predict(state)\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def store(self, state, action, reward, nstate, done):\n",
        "        #Store the experience in memory\n",
        "        self.memory.append( (state, action, reward, nstate, done) )\n",
        "\n",
        "    def expReplay(self, batch_size): ## training the neural network \n",
        "        #Execute the experience replay\n",
        "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
        "\n",
        "        #Convert to numpy for speed by vectorization\n",
        "        x = []\n",
        "        y = []\n",
        "        np_array = np.array(minibatch)\n",
        "        st = np.zeros((0, self.nS)) #States\n",
        "        nst = np.zeros( (0, self.nS) )#Next States\n",
        "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
        "            st = np.append( st, np_array[i,0], axis=0)\n",
        "            nst = np.append( nst, np_array[i,3], axis=0)\n",
        "        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n",
        "        nst_predict = self.model.predict(nst)\n",
        "        index = 0\n",
        "        for state, action, reward, nstate, done in minibatch:\n",
        "            x.append(state)\n",
        "            #Predict from state\n",
        "            nst_action_predict_model = nst_predict[index]\n",
        "            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n",
        "                target = reward\n",
        "            else:   #Non terminal\n",
        "                target = reward + self.gamma * np.amax(nst_action_predict_model)\n",
        "            target_f = st_predict[index]\n",
        "            target_f[action] = target\n",
        "            y.append(target_f)\n",
        "            index += 1\n",
        "        #Reshape for Keras Fit\n",
        "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
        "        y_reshape = np.array(y)\n",
        "        epoch_count = 1 #Epochs is the number or iterations\n",
        "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
        "        #Graph Losses\n",
        "        for i in range(epoch_count):\n",
        "            self.loss.append( hist.history['loss'][i] )\n",
        "        #Decay Epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "HXm9s5_LPxkv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oHyPgfDXpp"
      },
      "source": [
        "### **Train and build the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCqJzkeJtp3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c13bc2a-f62a-422a-f653-86304a2a986e"
      },
      "source": [
        "import sys\n",
        "\n",
        "if len(sys.argv) != 4:\n",
        "\tprint (\"Usage: python train.py [stock] [window] [episodes]\")\n",
        "\texit()\n",
        "\n",
        "\n",
        "stock_name = input(\"Enter stock_name, window_size, Episode_count\")\n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Training_Dataset\n",
        "#window_size = 10\n",
        "#Episode_count = 100 or it can be 10 or 20 or 30 and so on.\n",
        "\n",
        "window_size = input()\n",
        "episode_count = input()\n",
        "stock_name = str(stock_name)\n",
        "window_size = int(window_size)\n",
        "episode_count = int(episode_count)\n",
        "\n",
        "agent = Agent(window_size)\n",
        "data = getStockDataVec(stock_name)\n",
        "l = len(data) - 1\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(episode_count + 1):\n",
        "\tprint (\"Episode \" + str(e) + \"/\" + str(episode_count))\n",
        "\tstate = getState(data, 0, window_size + 1)\n",
        "\n",
        "\ttotal_profit = 0\n",
        "\tagent.inventory = []\n",
        "\n",
        "\tfor t in range(l):\n",
        "\t\taction = agent.act(state)\n",
        "\n",
        "\t\t# sit\n",
        "\t\tnext_state = getState(data, t + 1, window_size + 1)\n",
        "\t\treward = 0\n",
        "\n",
        "\t\tif action == 1: # buy\n",
        "\t\t\tagent.inventory.append(data[t])\n",
        "\t\t\t# print (\"Buy: \" + formatPrice(data[t]))\n",
        "\n",
        "\t\telif action == 2 and len(agent.inventory) > 0: # sell\n",
        "\t\t\tbought_price = agent.inventory.pop(0)\n",
        "\t\t\treward = max(data[t] - bought_price, 0)\n",
        "\t\t\ttotal_profit += data[t] - bought_price\n",
        "\t\t\t# print (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
        "\n",
        "\t\tdone = True if t == l - 1 else False\n",
        "\t\tagent.memory.append((state, action, reward, next_state, done))\n",
        "\t\tstate = next_state\n",
        "\n",
        "\t\tif done:\n",
        "\t\t\tprint (\"--------------------------------\")\n",
        "\t\t\tprint (\"-----Episode: {} -----\".format(e))\n",
        "\t\t\tprint (\"Total Profit: \" + formatPrice(total_profit))\n",
        "\t\t\t\n",
        "\n",
        "\t\tif len(agent.memory) > batch_size:\n",
        "\t\t\tagent.expReplay(batch_size)\n",
        "\n",
        "\t# # if e % 10 == 0:\n",
        "\tif e % 10 == 0:\n",
        "\t\tagent.model.save(\"model_ep\" + str(e))\n",
        "\t# agent.model.save(\"model_ep\" + str(e))\n",
        " \n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Training_Dataset\n",
        "#window_size = 10\n",
        "#Episode_count = 100 or it can be 10 or 20 or 30 and so on. "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: python train.py [stock] [window] [episodes]\n",
            "Enter stock_name, window_size, Episode_countGSPC_Training_Dataset\n",
            "10\n",
            "30\n",
            "Episode 0/30\n",
            "--------------------------------\n",
            "-----Episode: 0 -----\n",
            "Total Profit: $865.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/30\n",
            "--------------------------------\n",
            "-----Episode: 1 -----\n",
            "Total Profit: $7141.67\n",
            "Episode 2/30\n",
            "--------------------------------\n",
            "-----Episode: 2 -----\n",
            "Total Profit: $6871.84\n",
            "Episode 3/30\n",
            "--------------------------------\n",
            "-----Episode: 3 -----\n",
            "Total Profit: $6425.62\n",
            "Episode 4/30\n",
            "--------------------------------\n",
            "-----Episode: 4 -----\n",
            "Total Profit: $7306.68\n",
            "Episode 5/30\n",
            "--------------------------------\n",
            "-----Episode: 5 -----\n",
            "Total Profit: $7161.61\n",
            "Episode 6/30\n",
            "--------------------------------\n",
            "-----Episode: 6 -----\n",
            "Total Profit: $6990.42\n",
            "Episode 7/30\n",
            "--------------------------------\n",
            "-----Episode: 7 -----\n",
            "Total Profit: $7420.58\n",
            "Episode 8/30\n",
            "--------------------------------\n",
            "-----Episode: 8 -----\n",
            "Total Profit: $5741.95\n",
            "Episode 9/30\n",
            "--------------------------------\n",
            "-----Episode: 9 -----\n",
            "Total Profit: $7061.20\n",
            "Episode 10/30\n",
            "--------------------------------\n",
            "-----Episode: 10 -----\n",
            "Total Profit: $5241.04\n",
            "Episode 11/30\n",
            "--------------------------------\n",
            "-----Episode: 11 -----\n",
            "Total Profit: $7092.10\n",
            "Episode 12/30\n",
            "--------------------------------\n",
            "-----Episode: 12 -----\n",
            "Total Profit: $6816.25\n",
            "Episode 13/30\n",
            "--------------------------------\n",
            "-----Episode: 13 -----\n",
            "Total Profit: $6431.38\n",
            "Episode 14/30\n",
            "--------------------------------\n",
            "-----Episode: 14 -----\n",
            "Total Profit: $6240.14\n",
            "Episode 15/30\n",
            "--------------------------------\n",
            "-----Episode: 15 -----\n",
            "Total Profit: $5827.87\n",
            "Episode 16/30\n",
            "--------------------------------\n",
            "-----Episode: 16 -----\n",
            "Total Profit: $5912.93\n",
            "Episode 17/30\n",
            "--------------------------------\n",
            "-----Episode: 17 -----\n",
            "Total Profit: $4592.13\n",
            "Episode 18/30\n",
            "--------------------------------\n",
            "-----Episode: 18 -----\n",
            "Total Profit: $6146.69\n",
            "Episode 19/30\n",
            "--------------------------------\n",
            "-----Episode: 19 -----\n",
            "Total Profit: $6504.60\n",
            "Episode 20/30\n",
            "--------------------------------\n",
            "-----Episode: 20 -----\n",
            "Total Profit: $5526.26\n",
            "Episode 21/30\n",
            "--------------------------------\n",
            "-----Episode: 21 -----\n",
            "Total Profit: $6493.13\n",
            "Episode 22/30\n",
            "--------------------------------\n",
            "-----Episode: 22 -----\n",
            "Total Profit: $6761.81\n",
            "Episode 23/30\n",
            "--------------------------------\n",
            "-----Episode: 23 -----\n",
            "Total Profit: $6308.82\n",
            "Episode 24/30\n",
            "--------------------------------\n",
            "-----Episode: 24 -----\n",
            "Total Profit: $6131.78\n",
            "Episode 25/30\n",
            "--------------------------------\n",
            "-----Episode: 25 -----\n",
            "Total Profit: $6599.88\n",
            "Episode 26/30\n",
            "--------------------------------\n",
            "-----Episode: 26 -----\n",
            "Total Profit: $5144.48\n",
            "Episode 27/30\n",
            "--------------------------------\n",
            "-----Episode: 27 -----\n",
            "Total Profit: $5699.61\n",
            "Episode 28/30\n",
            "--------------------------------\n",
            "-----Episode: 28 -----\n",
            "Total Profit: $5480.21\n",
            "Episode 29/30\n",
            "--------------------------------\n",
            "-----Episode: 29 -----\n",
            "Total Profit: $5844.89\n",
            "Episode 30/30\n",
            "--------------------------------\n",
            "-----Episode: 30 -----\n",
            "Total Profit: $6726.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save all the model to my google drive\n",
        "!scp -r /content/model_ep* /content/drive/MyDrive/datasets/simplilearn_RL_stock_trading/model_file"
      ],
      "metadata": {
        "id": "uKqTAFzJNzdm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcXCrJUSDXpr"
      },
      "source": [
        "### **Evaluate the model and agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmZUVXe5t95k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad59ce00-b93c-424c-aa72-21a16b1a0111"
      },
      "source": [
        "import sys\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "if len(sys.argv) != 3:\n",
        "\tprint (\"Usage: python evaluate.py [stock] [model]\")\n",
        "\texit()\n",
        "\n",
        "\n",
        "stock_name = input(\"Enter Stock_name, Model_name\")\n",
        "model_name = input()\n",
        "#Note: \n",
        "#Fill the given information when prompted: \n",
        "#Enter stock_name = GSPC_Evaluation_Dataset\n",
        "#Model_name = respective model name\n",
        "\n",
        "model = load_model(\"\" + model_name)\n",
        "window_size = model.layers[0].input.shape.as_list()[1]\n",
        "\n",
        "agent = Agent(window_size, True, model_name)\n",
        "data = getStockDataVec(stock_name)\n",
        "l = len(data) - 1\n",
        "batch_size = 32\n",
        "\n",
        "state = getState(data, 0, window_size + 1)\n",
        "total_profit = 0\n",
        "agent.inventory = []\n",
        "\n",
        "for t in range(l):\n",
        "\taction = agent.act(state)\n",
        "\n",
        "\t# sit\n",
        "\tnext_state = getState(data, t + 1, window_size + 1)\n",
        "\treward = 0\n",
        "\n",
        "\tif action == 1: # buy\n",
        "\t\tagent.inventory.append(data[t])\n",
        "\t\tprint (\"Buy: \" + formatPrice(data[t]))\n",
        "\n",
        "\telif action == 2 and len(agent.inventory) > 0: # sell\n",
        "\t\tbought_price = agent.inventory.pop(0)\n",
        "\t\treward = max(data[t] - bought_price, 0)\n",
        "\t\ttotal_profit += data[t] - bought_price\n",
        "\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
        "\n",
        "\tdone = True if t == l - 1 else False\n",
        "\tagent.memory.append((state, action, reward, next_state, done))\n",
        "\tstate = next_state\n",
        "\n",
        "\tif done:\n",
        "\t\tprint (\"--------------------------------\")\n",
        "\t\t# print\t(\"-----Episode: {} -----\".format(e))\n",
        "\t\tprint (stock_name + \" Total Profit: \" + formatPrice(total_profit))\n",
        "\n",
        "\t# if len(agent.memory) > batch_size:\n",
        "\t# \tagent.expReplay(batch_size)\n",
        "\n",
        "# GSPC_Evaluation_Dataset\n",
        "# /content/model_ep30"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Stock_name, Model_nameGSPC_Evaluation_Dataset\n",
            "/content/model_ep30\n",
            "Buy: $1271.87\n",
            "Buy: $1276.56\n",
            "Buy: $1273.85\n",
            "Buy: $1271.50\n",
            "Buy: $1269.75\n",
            "Buy: $1285.96\n",
            "Sell: $1293.24 | Profit: $21.37\n",
            "Buy: $1295.02\n",
            "Sell: $1281.92 | Profit: $5.36\n",
            "Sell: $1280.26 | Profit: $6.41\n",
            "Buy: $1283.35\n",
            "Buy: $1291.18\n",
            "Buy: $1296.63\n",
            "Sell: $1276.34 | Profit: $4.84\n",
            "Sell: $1286.12 | Profit: $16.37\n",
            "Sell: $1307.10 | Profit: $21.14\n",
            "Buy: $1310.87\n",
            "Buy: $1319.05\n",
            "Sell: $1324.57 | Profit: $29.55\n",
            "Buy: $1320.88\n",
            "Sell: $1321.87 | Profit: $38.52\n",
            "Buy: $1329.15\n",
            "Sell: $1328.01 | Profit: $36.83\n",
            "Sell: $1343.01 | Profit: $46.38\n",
            "Sell: $1315.44 | Profit: $4.57\n",
            "Sell: $1307.40 | Profit: -$11.65\n",
            "Buy: $1306.10\n",
            "Buy: $1319.88\n",
            "Sell: $1327.22 | Profit: $6.34\n",
            "Sell: $1306.33 | Profit: -$22.82\n",
            "Sell: $1330.97 | Profit: $24.87\n",
            "Sell: $1321.15 | Profit: $1.27\n",
            "Buy: $1310.13\n",
            "Buy: $1321.82\n",
            "Buy: $1320.02\n",
            "Buy: $1295.11\n",
            "Buy: $1304.28\n",
            "Sell: $1296.39 | Profit: -$13.74\n",
            "Buy: $1281.87\n",
            "Sell: $1256.88 | Profit: -$64.94\n",
            "Buy: $1298.38\n",
            "Sell: $1293.77 | Profit: -$26.25\n",
            "Sell: $1297.54 | Profit: $2.43\n",
            "Buy: $1319.44\n",
            "Sell: $1328.26 | Profit: $23.98\n",
            "Buy: $1325.83\n",
            "Sell: $1332.41 | Profit: $50.54\n",
            "Buy: $1332.87\n",
            "Sell: $1335.54 | Profit: $37.16\n",
            "Buy: $1333.51\n",
            "Buy: $1324.46\n",
            "Sell: $1314.52 | Profit: -$4.92\n",
            "Sell: $1319.68 | Profit: -$6.15\n",
            "Sell: $1305.14 | Profit: -$27.73\n",
            "Buy: $1312.62\n",
            "Sell: $1330.36 | Profit: -$3.15\n",
            "Buy: $1337.38\n",
            "Sell: $1347.24 | Profit: $22.78\n",
            "Buy: $1355.66\n",
            "Buy: $1360.48\n",
            "Buy: $1363.61\n",
            "Sell: $1361.22 | Profit: $48.60\n",
            "Buy: $1356.62\n",
            "Sell: $1347.32 | Profit: $9.94\n",
            "Sell: $1340.20 | Profit: -$15.46\n",
            "Buy: $1346.29\n",
            "Buy: $1357.16\n",
            "Sell: $1342.08 | Profit: -$18.40\n",
            "Sell: $1328.98 | Profit: -$34.63\n",
            "Sell: $1340.68 | Profit: -$15.94\n",
            "Buy: $1343.60\n",
            "Buy: $1333.27\n",
            "Buy: $1317.37\n",
            "Sell: $1320.47 | Profit: -$25.82\n",
            "Buy: $1331.10\n",
            "Sell: $1286.17 | Profit: -$70.99\n",
            "Buy: $1279.56\n",
            "Sell: $1271.83 | Profit: -$71.77\n",
            "Buy: $1287.87\n",
            "Buy: $1267.64\n",
            "Buy: $1271.50\n",
            "Buy: $1295.52\n",
            "Sell: $1287.14 | Profit: -$46.13\n",
            "Buy: $1296.67\n",
            "Buy: $1320.64\n",
            "Sell: $1339.67 | Profit: $22.30\n",
            "Buy: $1337.88\n",
            "Sell: $1339.22 | Profit: $8.12\n",
            "Buy: $1353.22\n",
            "Sell: $1319.49 | Profit: $39.93\n",
            "Sell: $1313.64 | Profit: $25.77\n",
            "Sell: $1317.72 | Profit: $50.08\n",
            "Sell: $1308.87 | Profit: $37.37\n",
            "Buy: $1316.14\n",
            "Sell: $1326.73 | Profit: $31.21\n",
            "Sell: $1325.84 | Profit: $29.17\n",
            "Sell: $1343.80 | Profit: $23.16\n",
            "Sell: $1345.02 | Profit: $7.14\n",
            "Sell: $1337.43 | Profit: -$15.79\n",
            "Buy: $1331.94\n",
            "Sell: $1304.89 | Profit: -$11.25\n",
            "Sell: $1300.67 | Profit: -$31.27\n",
            "Buy: $1286.94\n",
            "Buy: $1260.34\n",
            "Sell: $1199.38 | Profit: -$87.56\n",
            "Sell: $1172.53 | Profit: -$87.81\n",
            "Buy: $1178.81\n",
            "Buy: $1193.89\n",
            "Buy: $1140.65\n",
            "Buy: $1123.82\n",
            "Sell: $1162.35 | Profit: -$16.46\n",
            "Sell: $1159.27 | Profit: -$34.62\n",
            "Sell: $1176.80 | Profit: $36.15\n",
            "Sell: $1210.08 | Profit: $86.26\n",
            "Buy: $1204.42\n",
            "Sell: $1165.24 | Profit: -$39.18\n",
            "Buy: $1198.62\n",
            "Buy: $1185.90\n",
            "Buy: $1172.87\n",
            "Buy: $1209.11\n",
            "Buy: $1204.09\n",
            "Sell: $1202.09 | Profit: $3.47\n",
            "Sell: $1129.56 | Profit: -$56.34\n",
            "Sell: $1136.43 | Profit: -$36.44\n",
            "Buy: $1175.38\n",
            "Buy: $1151.06\n",
            "Buy: $1160.40\n",
            "Buy: $1131.42\n",
            "Sell: $1123.95 | Profit: -$85.16\n",
            "Buy: $1144.03\n",
            "Buy: $1155.46\n",
            "Sell: $1207.25 | Profit: $3.16\n",
            "Sell: $1203.66 | Profit: $28.28\n",
            "Buy: $1224.58\n",
            "Sell: $1209.88 | Profit: $58.82\n",
            "Buy: $1215.39\n",
            "Sell: $1254.19 | Profit: $93.79\n",
            "Buy: $1229.05\n",
            "Buy: $1242.00\n",
            "Sell: $1284.59 | Profit: $153.17\n",
            "Buy: $1253.30\n",
            "Buy: $1237.90\n",
            "Buy: $1261.15\n",
            "Sell: $1253.23 | Profit: $109.20\n",
            "Sell: $1261.12 | Profit: $105.66\n",
            "Sell: $1229.10 | Profit: $4.52\n",
            "Sell: $1251.78 | Profit: $36.39\n",
            "Sell: $1257.81 | Profit: $28.76\n",
            "Buy: $1216.13\n",
            "Buy: $1192.98\n",
            "Sell: $1188.04 | Profit: -$53.96\n",
            "Sell: $1161.79 | Profit: -$91.51\n",
            "Sell: $1158.67 | Profit: -$79.23\n",
            "Buy: $1195.19\n",
            "Sell: $1246.96 | Profit: -$14.19\n",
            "Sell: $1244.58 | Profit: $28.45\n",
            "Sell: $1261.01 | Profit: $68.03\n",
            "Buy: $1234.35\n",
            "Sell: $1255.19 | Profit: $60.00\n",
            "Buy: $1236.47\n",
            "Sell: $1225.73 | Profit: -$8.62\n",
            "Sell: $1211.82 | Profit: -$24.65\n",
            "Buy: $1215.75\n",
            "Buy: $1219.66\n",
            "Sell: $1243.72 | Profit: $27.97\n",
            "Sell: $1254.00 | Profit: $34.34\n",
            "--------------------------------\n",
            "GSPC_Evaluation_Dataset Total Profit: $445.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIolgCRWSM-9"
      },
      "source": [
        "**Note: Run the training section for considerable episodes so that while evaluating the model it can generate significant profit.** \n"
      ]
    }
  ]
}