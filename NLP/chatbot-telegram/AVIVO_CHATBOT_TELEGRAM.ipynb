{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx4655uV1tVw",
        "outputId": "1195eb42-1f38-41db-9be4-a09f58c64480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/731.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m256.0/731.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.0/731.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/138.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q python-telegram-bot sentence-transformers groq nest-asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "from telegram import Update\n",
        "from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyZdU6ba1wBt",
        "outputId": "3b7c0831-dd07-4559-87a1-af19e2c1367f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace with your actual tokens\n",
        "TELEGRAM_TOKEN = \"TELEGRAM TOKEN PASTE IT\"\n",
        "GROQ_API_KEY = \"GROQ API PASTE IT\"  # Get free key from https://console.groq.com\n"
      ],
      "metadata": {
        "id": "Yk1JcvNw1xwf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = {\n",
        "    \"policy\": \"\"\"Company Leave Policy:\n",
        "Employees are entitled to 20 paid leaves per year.\n",
        "Sick leave can be taken anytime with medical proof.\n",
        "Remote work allowed up to 3 days per week.\"\"\",\n",
        "\n",
        "    \"faq\": \"\"\"FAQ:\n",
        "Q: What is the refund policy?\n",
        "A: Refunds are allowed within 30 days of purchase.\n",
        "\n",
        "Q: Do you provide tech support?\n",
        "A: Yes, 24/7 email support is available.\"\"\",\n",
        "\n",
        "    \"recipes\": \"\"\"Recipe: Pancakes\n",
        "Ingredients: Flour, Milk, Eggs, Sugar, Butter\n",
        "Steps: Mix ingredients, heat pan, cook both sides.\"\"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "a3Hgob0313gj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading embedding model...\")\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "print(\"Initializing Groq client...\")\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4R5CVS116f5",
        "outputId": "9b882ba5-02d2-4a55-f493-0747e39ac8ce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Initializing Groq client...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Build Vector Store\n",
        "# ============================================\n",
        "\n",
        "def build_store(docs):\n",
        "    texts, vectors = [], []\n",
        "    for t in docs.values():\n",
        "        # Split into chunks of 300 characters\n",
        "        for i in range(0, len(t), 300):\n",
        "            chunk = t[i:i+300]\n",
        "            texts.append(chunk)\n",
        "            vectors.append(embedding_model.encode(chunk))\n",
        "\n",
        "    return texts, np.vstack(vectors)\n",
        "\n",
        "print(\"Building vector store...\")\n",
        "store_texts, store_vectors = build_store(docs)\n",
        "print(f\"Vector store ready with {len(store_texts)} chunks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgfqKHdA18sd",
        "outputId": "d98b5412-30a8-4212-d385-bdce9f65445c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vector store...\n",
            "Vector store ready with 3 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# RAG Functions\n",
        "# ============================================\n",
        "\n",
        "def retrieve(query, k=3):\n",
        "    \"\"\"Retrieve top-k most relevant document chunks\"\"\"\n",
        "    q = embedding_model.encode(query)\n",
        "    sims = store_vectors @ q / (np.linalg.norm(store_vectors, axis=1) * np.linalg.norm(q))\n",
        "    idx = np.argsort(sims)[-k:][::-1]\n",
        "    return [store_texts[i] for i in idx]"
      ],
      "metadata": {
        "id": "BBznAH_R2BxY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(context, question):\n",
        "    \"\"\"Generate answer using Groq LLM\"\"\"\n",
        "    try:\n",
        "        prompt = f\"\"\"You are a helpful assistant. Answer the question based on the provided context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer the question clearly and concisely based only on the context provided.\"\"\"\n",
        "\n",
        "        response = groq_client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",  # Fast and accurate model\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on provided context.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {str(e)}\""
      ],
      "metadata": {
        "id": "nBOvXzdf2ETq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Telegram Bot Handlers\n",
        "# ============================================\n",
        "\n",
        "history = {}\n",
        "\n",
        "async def start_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    \"\"\"Handle /start command\"\"\"\n",
        "    welcome_msg = \"\"\"Welcome to the RAG Bot!\n",
        "\n",
        "I can answer questions about:\n",
        "â€¢ Company leave policy\n",
        "â€¢ FAQ (refunds, tech support)\n",
        "â€¢ Recipes\n",
        "\n",
        "Commands:\n",
        "/ask <your question> - Ask me anything\n",
        "/help - Show this help message\n",
        "\n",
        "Example: /ask what is the refund policy?\"\"\"\n",
        "\n",
        "    await update.message.reply_text(welcome_msg)\n",
        "\n",
        "async def help_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    \"\"\"Handle /help command\"\"\"\n",
        "    help_msg = \"\"\"How to use this bot:\n",
        "\n",
        "/ask <question> - Ask the bot a question\n",
        "/help - Show this help message\n",
        "\n",
        "Example questions:\n",
        "â€¢ /ask what is the refund policy?\n",
        "â€¢ /ask how many leaves do employees get?\n",
        "â€¢ /ask how to make pancakes?\"\"\"\n",
        "\n",
        "    await update.message.reply_text(help_msg)\n",
        "\n",
        "async def ask_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    \"\"\"Handle /ask command\"\"\"\n",
        "    try:\n",
        "        user = update.effective_user.id\n",
        "        query = \" \".join(context.args)\n",
        "\n",
        "        # Check if query is empty\n",
        "        if not query:\n",
        "            await update.message.reply_text(\n",
        "                \"Please provide a question!\\n\\nExample: /ask what is the refund policy?\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "        print(f\"User {user} asked: {query}\")\n",
        "\n",
        "        # Send thinking message\n",
        "        thinking_msg = await update.message.reply_text(\"Searching and generating answer...\")\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = retrieve(query)\n",
        "        ctx = \"\\n\\n\".join(docs)\n",
        "\n",
        "        print(f\"Retrieved {len(docs)} relevant chunks\")\n",
        "\n",
        "        # Generate answer using Groq\n",
        "        answer = generate_answer(ctx, query)\n",
        "\n",
        "        # Store in history\n",
        "        history.setdefault(user, []).append((query, answer))\n",
        "        history[user] = history[user][-5:]  # Keep last 5 interactions\n",
        "\n",
        "        # Send answer\n",
        "        await thinking_msg.edit_text(f\"ğŸ’¬ **Answer:**\\n\\n{answer}\")\n",
        "\n",
        "        print(f\"Answered user {user}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Sorry, an error occurred: {str(e)}\"\n",
        "        print(f\"Error: {e}\")\n",
        "        await update.message.reply_text(error_msg)"
      ],
      "metadata": {
        "id": "W3DpJEGU2HRu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Bot Setup and Run\n",
        "# ============================================\n",
        "\n",
        "# Build application\n",
        "app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()\n",
        "\n",
        "# Add command handlers\n",
        "app.add_handler(CommandHandler(\"start\", start_cmd))\n",
        "app.add_handler(CommandHandler(\"help\", help_cmd))\n",
        "app.add_handler(CommandHandler(\"ask\", ask_cmd))\n",
        "\n",
        "async def run_bot():\n",
        "    \"\"\"Run the bot with manual polling\"\"\"\n",
        "    print(\"Bot starting...\")\n",
        "    await app.initialize()\n",
        "    await app.start()\n",
        "    print(\"Bot is now running!\")\n",
        "    print(\"Send /start to your bot on Telegram to begin\")\n",
        "    print(\"\\nPress Ctrl+C in Colab to stop the bot\\n\")\n",
        "\n",
        "    offset = None\n",
        "    try:\n",
        "        while True:\n",
        "            updates = await app.bot.get_updates(offset=offset, timeout=10)\n",
        "            for u in updates:\n",
        "                offset = u.update_id + 1\n",
        "                await app.process_update(u)\n",
        "            await asyncio.sleep(0.5)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nStopping bot...\")\n",
        "        await app.stop()\n",
        "        print(\"Bot stopped\")\n",
        "\n",
        "# Run the bot\n",
        "await run_bot()"
      ],
      "metadata": {
        "id": "fl_w62lX2MGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFJe-iAa2PV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}